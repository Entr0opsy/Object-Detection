{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Importing Necessary Libraries**"
      ],
      "metadata": {
        "id": "iY6VSiPWzuun"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "69U_nn-SzJUW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining the Convolutional Neural Network Architecture**"
      ],
      "metadata": {
        "id": "TW5T3ocEz23H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a CNN class which is a subclass of nn.Module\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # Define the convolutional layers and a dropout layer\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)  # 1 input channel, 10 output channels\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)  # 10 input channels, 20 output channels\n",
        "        self.conv2_drop = nn.Dropout2d()  # Dropout layer to reduce overfitting\n",
        "        # Define fully connected layers\n",
        "        self.fc1 = nn.Linear(320, 50)  # 320 input features, 50 output features\n",
        "        self.fc2 = nn.Linear(50, 10)  # 50 input features, 10 output features (for 10 classes)\n",
        "\n",
        "    # Define the forward pass\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))  # Apply ReLU activation and max pooling to the first conv layer's output\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))  # Apply conv layer, dropout, ReLU activation and max pooling\n",
        "        x = x.view(-1, 320)  # Flatten the tensor for the fully connected layer\n",
        "        x = F.relu(self.fc1(x))  # Apply ReLU activation to the first fully connected layer\n",
        "        x = F.dropout(x, training=self.training)  # Apply dropout\n",
        "        x = self.fc2(x)  # Pass through the second fully connected layer\n",
        "        return F.log_softmax(x, dim=1)  # Apply log softmax to get log-probabilities\n",
        "\n",
        "# Create an instance of the Net class\n",
        "net = Net()\n"
      ],
      "metadata": {
        "id": "6APwy9I1zVR2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading the Data and Defining Hyperparameters**"
      ],
      "metadata": {
        "id": "LZiv-M7Dz9O3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set batch size for the dataloaders and define learning rate and number of epochs for training\n",
        "batch_size = 64\n",
        "learning_rate = 0.01\n",
        "epochs = 5\n",
        "\n",
        "# Transformations to be applied to the images\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
        "    transforms.Normalize((0.1307,), (0.3081,))  # Normalize the images\n",
        "])\n",
        "\n",
        "# Load the MNIST training set\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
        "                                      download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True)\n",
        "\n",
        "# Load the MNIST test set\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
        "                                     download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nO3FAN9mzg4e",
        "outputId": "ea9d7f81-3c1a-466f-e762-b8c2dacfbae2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 11549421.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 344177.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 3164023.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3099158.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training the Network**"
      ],
      "metadata": {
        "id": "UpMH9W230Frx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()  # Loss function for multi-class classification\n",
        "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)  # Stochastic Gradient Descent as the optimizer\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):  # Loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data  # Get the input data and the labels\n",
        "\n",
        "        optimizer.zero_grad()  # Zero the parameter gradients to avoid accumulation\n",
        "\n",
        "        outputs = net(inputs)  # Forward pass: compute the predicted outputs\n",
        "        loss = criterion(outputs, labels)  # Compute the loss\n",
        "        loss.backward()  # Backward pass: compute the gradient of the loss w.r.t the model's weights\n",
        "        optimizer.step()  # Perform an optimization step to update the weights\n",
        "\n",
        "        running_loss += loss.item()  # Accumulate the loss\n",
        "        if i % 100 == 99:    # Print every 100 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_AO3DqtzoA9",
        "outputId": "ed9be55f-7a97-4d40-ee8e-0013a67a6a54"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   100] loss: 1.671\n",
            "[1,   200] loss: 0.730\n",
            "[1,   300] loss: 0.538\n",
            "[1,   400] loss: 0.444\n",
            "[1,   500] loss: 0.414\n",
            "[1,   600] loss: 0.367\n",
            "[1,   700] loss: 0.344\n",
            "[1,   800] loss: 0.318\n",
            "[1,   900] loss: 0.319\n",
            "[2,   100] loss: 0.278\n",
            "[2,   200] loss: 0.275\n",
            "[2,   300] loss: 0.282\n",
            "[2,   400] loss: 0.273\n",
            "[2,   500] loss: 0.265\n",
            "[2,   600] loss: 0.253\n",
            "[2,   700] loss: 0.254\n",
            "[2,   800] loss: 0.234\n",
            "[2,   900] loss: 0.222\n",
            "[3,   100] loss: 0.203\n",
            "[3,   200] loss: 0.220\n",
            "[3,   300] loss: 0.206\n",
            "[3,   400] loss: 0.203\n",
            "[3,   500] loss: 0.221\n",
            "[3,   600] loss: 0.202\n",
            "[3,   700] loss: 0.189\n",
            "[3,   800] loss: 0.201\n",
            "[3,   900] loss: 0.203\n",
            "[4,   100] loss: 0.196\n",
            "[4,   200] loss: 0.185\n",
            "[4,   300] loss: 0.186\n",
            "[4,   400] loss: 0.181\n",
            "[4,   500] loss: 0.189\n",
            "[4,   600] loss: 0.194\n",
            "[4,   700] loss: 0.192\n",
            "[4,   800] loss: 0.197\n",
            "[4,   900] loss: 0.194\n",
            "[5,   100] loss: 0.173\n",
            "[5,   200] loss: 0.190\n",
            "[5,   300] loss: 0.183\n",
            "[5,   400] loss: 0.167\n",
            "[5,   500] loss: 0.174\n",
            "[5,   600] loss: 0.176\n",
            "[5,   700] loss: 0.163\n",
            "[5,   800] loss: 0.172\n",
            "[5,   900] loss: 0.169\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Z2lUDda50xPx"
      }
    }
  ]
}