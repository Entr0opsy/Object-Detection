{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CC9Q0Ake6K92",
        "outputId": "08892d28-19be-454f-d968-a66fc5ca517c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive to Google Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "image_dir = \"/content/drive/My Drive/helmet-dataset/images\"\n",
        "anno_dir = \"/content/drive/My Drive/helmet-dataset/annotations\"\n",
        "\n",
        "path_annotations = []\n",
        "for i in Path(anno_dir).glob('*.xml'):\n",
        "    path_annotations.append(i)\n",
        "path_annotations = sorted(path_annotations) #contains path to 5000 annots\n",
        "\n",
        "path_images = []\n",
        "for i in Path(image_dir).glob('*.png'):\n",
        "    path_images.append(i)\n",
        "path_images = sorted(path_images) #contains path to 5000 images\n",
        "\n",
        "\n",
        "# Split data : 90% Train, 10% Val\n",
        "path_train_annot = path_annotations[:4000]\n",
        "path_train_images = path_images[:4000]\n",
        "\n",
        "path_val_annot = path_annotations[4000:4500]\n",
        "path_val_images = path_images[4000:4500]\n",
        "\n",
        "path_test_annot = path_annotations[4500:5000]\n",
        "path_test_images = path_images[4500:5000]\n",
        "\n",
        "\n",
        "# Creating directories to put train & val data\n",
        "os.makedirs('/content/drive/My Drive/helmet_dataset/train/annotations',exist_ok = True)\n",
        "os.makedirs('/content/drive/My Drive/helmet_dataset/train/images', exist_ok = True)\n",
        "os.makedirs('/content/drive/My Drive/helmet_dataset/val/annotations', exist_ok = True)\n",
        "os.makedirs('/content/drive/My Drive/helmet_dataset/val/images', exist_ok = True)\n",
        "os.makedirs('/content/drive/My Drive/helmet_dataset/test/annotations', exist_ok = True)\n",
        "os.makedirs('/content/drive/My Drive/helmet_dataset/test/images', exist_ok = True)\n",
        "os.makedirs('/content/drive/My Drive/helmet_dataset/savedmodel', exist_ok = True)\n",
        "\n",
        "\n",
        "#Copy data into train and val folders\n",
        "for i, (path_annot, path_img) in tqdm(enumerate(zip(path_train_annot, path_train_images))):\n",
        "    shutil.copy(path_img, '/content/drive/My Drive/helmet_dataset/train/images/' + path_img.parts[-1])\n",
        "    shutil.copy(path_annot, '/content/drive/My Drive/helmet_dataset/train/annotations/' + path_annot.parts[-1])\n",
        "\n",
        "for i, (path_annot, path_img) in tqdm(enumerate(zip(path_val_annot, path_val_images))):\n",
        "    shutil.copy(path_img, '/content/drive/My Drive/helmet_dataset/val/images/' + path_img.parts[-1])\n",
        "    shutil.copy(path_annot, '/content/drive/My Drive/helmet_dataset/val/annotations/' + path_annot.parts[-1])\n",
        "\n",
        "for i, (path_annot, path_img) in tqdm(enumerate(zip(path_test_annot, path_test_images))):\n",
        "    shutil.copy(path_img, '/content/drive/My Drive/helmet_dataset/test/images/' + path_img.parts[-1])\n",
        "    shutil.copy(path_annot, '/content/drive/My Drive/helmet_dataset/test/annotations/' + path_annot.parts[-1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oWJiwny6Rg8",
        "outputId": "23d545d5-c8be-44a8-ca1b-fe9daf811d04"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "BATCH_SIZE = 8 #increase / decrease according to memory\n",
        "NUM_EPOCHS = 10 # number of epochs to train for\n",
        "NUM_WORKERS = 2\n",
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# training images and XML files directory\n",
        "TRAIN_DIR = '/content/drive/My Drive/helmet_dataset/train'\n",
        "# validation images and XML files directory\n",
        "VALID_DIR = '/content/drive/My Drive/helmet_dataset/val'\n",
        "\n",
        "# classes: 0 index is reserved for background\n",
        "CLASS_NAME = ['__background__', 'helmet', 'head', 'person']\n",
        "NUM_CLASSES = len(CLASS_NAME)\n"
      ],
      "metadata": {
        "id": "ToYjAd5D6bY9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply transform to image\n",
        "import albumentations as A #new method, customizable transformations\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# define the training tranforms\n",
        "def get_train_transform():\n",
        "    return A.Compose([\n",
        "        A.Resize(415,415),\n",
        "        A.Blur(blur_limit=3, p=0.1),\n",
        "        ToTensorV2(p=1.0),\n",
        "        ], bbox_params={'format': 'pascal_voc','label_fields': ['labels']}\n",
        "    )\n",
        "\n",
        "# define the validation transforms (validation data shud not be augmented)\n",
        "def get_valid_transform():\n",
        "    return A.Compose([\n",
        "        A.Resize(415,415),\n",
        "        ToTensorV2(p=1.0),\n",
        "        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']}\n",
        "    )\n",
        "\n",
        "# The collate_fn() will help us take care of tensors of varying sizes while creating\n",
        "# the training and validation data loaders.\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    To handle the data loading as different images may have different number\n",
        "    of objects and to handle varying size tensors as well.\n",
        "    \"\"\"\n",
        "    return tuple(zip(*batch))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znQguL6s6drU",
        "outputId": "6d66adf1-9694-4a14-e20e-3a6c21cbffa6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.15 (you have 1.4.14). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the pytorch dataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import glob, matplotlib.pyplot as plt, matplotlib.patches as patches\n",
        "import xml.etree.ElementTree as ET\n",
        "from pathlib import Path\n",
        "\n",
        "# foll is the class to create pytorch dataset from images(.png) & annotations(.xml)\n",
        "class SafetyHelmDataset(Dataset):\n",
        "    def __init__(self, dir_path, class_name, transforms=None):\n",
        "        self.dir_path = dir_path\n",
        "        self.class_name = class_name\n",
        "        self.transforms = transforms\n",
        "\n",
        "        # get all the image paths in sorted order\n",
        "        self.image_paths = glob.glob(f\"{self.dir_path}/images/*.png\")\n",
        "        self.all_images = sorted(self.image_paths)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        image_name = self.all_images[idx] #reading image one by one\n",
        "\n",
        "        image = plt.imread(image_name) #read the image\n",
        "        image /= 255.0 #normalize pixels b/w 0 & 1\n",
        "\n",
        "        # capture the corresponding xml file for getting annotations\n",
        "        annot_filepath = os.path.join(f'{self.dir_path}/annotations/', Path(image_name).parts[-1][:-4] + '.xml')\n",
        "\n",
        "        boxes, labels = [], []\n",
        "        root = ET.parse(annot_filepath).getroot() #reading xml file\n",
        "\n",
        "        # extracting box coordinates from xml annotations\n",
        "        for member in root.findall('object'):\n",
        "            # map the current object name to classes to get the label index\n",
        "            labels.append(self.class_name.index(member.find('name').text))\n",
        "\n",
        "            # (xmin, ymin) are left corner coordinates & (xmax, ymax) are right corner coordinates\n",
        "            xmin = int(member.find('bndbox').find('xmin').text)\n",
        "            ymin = int(member.find('bndbox').find('ymin').text)\n",
        "            xmax = int(member.find('bndbox').find('xmax').text)\n",
        "            ymax = int(member.find('bndbox').find('ymax').text)\n",
        "\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "        # bounding box to tensor\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "\n",
        "        # labels to tensor\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        # prepare the final target dict\n",
        "        target = {}\n",
        "        target['boxes'] = boxes\n",
        "        target['labels'] = labels\n",
        "        image_id = torch.tensor([idx])\n",
        "        target['image_id'] = image_id\n",
        "\n",
        "        # apply image transforms\n",
        "        if self.transforms:\n",
        "            sample = self.transforms(image=image, bboxes=target['boxes'], labels=labels)\n",
        "            image = sample['image']\n",
        "            target['boxes'] = torch.Tensor(sample['bboxes'])\n",
        "\n",
        "        return image, target #these are the o/p of this class\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_images)\n"
      ],
      "metadata": {
        "id": "B2kJb-7B6iJj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = SafetyHelmDataset(TRAIN_DIR, CLASS_NAME, get_train_transform())\n",
        "valid_dataset = SafetyHelmDataset(VALID_DIR, CLASS_NAME, get_valid_transform())\n"
      ],
      "metadata": {
        "id": "j3mAJzje6lAH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Aw4hybSj6nfs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}